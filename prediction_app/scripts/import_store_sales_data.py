#!/usr/bin/env python3
"""
å°† store_sales_data.csv å¯¼å…¥åˆ° PostgreSQL æ•°æ®åº“
"""
import os
import sys
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
script_dir = Path(__file__).parent.absolute()
prediction_app_dir = script_dir.parent.absolute()
project_root = prediction_app_dir.parent.absolute()
env_path = prediction_app_dir / ".env"
if env_path.exists():
    load_dotenv(env_path)
    logger.info(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    logger.warning(f"âš ï¸  ç¯å¢ƒå˜é‡æ–‡ä»¶ä¸å­˜åœ¨: {env_path}")


def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        conn = psycopg2.connect(
            host=os.getenv("DB_HOST"),
            port=os.getenv("DB_PORT", 5432),
            database=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        return conn
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise


def create_table(conn):
    """åˆ›å»º store_sales_data è¡¨"""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS store_sales_data (
        id SERIAL PRIMARY KEY,
        row_id INTEGER,
        order_id VARCHAR(50),
        order_date DATE,
        ship_date DATE,
        ship_mode VARCHAR(50),
        customer_id VARCHAR(50),
        customer_name VARCHAR(200),
        segment VARCHAR(50),
        country VARCHAR(100),
        city VARCHAR(100),
        state VARCHAR(100),
        postal_code VARCHAR(20),
        region VARCHAR(50),
        product_id VARCHAR(50),
        category VARCHAR(50),
        sub_category VARCHAR(50),
        product_name TEXT,
        sales DECIMAL(10, 2),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    
    CREATE INDEX IF NOT EXISTS idx_order_id ON store_sales_data(order_id);
    CREATE INDEX IF NOT EXISTS idx_customer_id ON store_sales_data(customer_id);
    CREATE INDEX IF NOT EXISTS idx_order_date ON store_sales_data(order_date);
    CREATE INDEX IF NOT EXISTS idx_product_id ON store_sales_data(product_id);
    CREATE INDEX IF NOT EXISTS idx_category ON store_sales_data(category);
    CREATE INDEX IF NOT EXISTS idx_region ON store_sales_data(region);
    """
    
    try:
        with conn.cursor() as cur:
            cur.execute(create_table_sql)
            conn.commit()
            logger.info("âœ… è¡¨ store_sales_data åˆ›å»ºæˆåŠŸï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰")
    except Exception as e:
        conn.rollback()
        logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
        raise


def parse_date(date_str):
    """è§£ææ—¥æœŸå­—ç¬¦ä¸² - æ ¼å¼ï¼šMM/DD/YYYY"""
    try:
        if pd.isna(date_str):
            return None
        # æ ¼å¼ï¼š08/11/2017
        return pd.to_datetime(date_str, format='%m/%d/%Y').date()
    except:
        try:
            return pd.to_datetime(date_str).date()
        except:
            return None


def import_data(conn, csv_path, batch_size=10000):
    """å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
    logger.info(f"ğŸ“‚ å¼€å§‹è¯»å– CSV æ–‡ä»¶: {csv_path}")
    
    # è¯»å– CSV æ–‡ä»¶ï¼Œå°è¯•ä¸åŒçš„ç¼–ç 
    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
    df = None
    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, low_memory=False, encoding=encoding)
            logger.info(f"âœ… ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸè¯»å–æ–‡ä»¶")
            break
        except UnicodeDecodeError:
            continue
    
    if df is None:
        logger.warning("âš ï¸  å°è¯•ä½¿ç”¨ errors='replace' è¯»å–æ–‡ä»¶")
        df = pd.read_csv(csv_path, low_memory=False, encoding='utf-8', errors='replace')
    
    logger.info(f"ğŸ“Š è¯»å–åˆ° {len(df)} è¡Œæ•°æ®")
    
    # æ•°æ®é¢„å¤„ç†
    logger.info("ğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    
    # å¤„ç†æ—¥æœŸ
    df['Order Date'] = df['Order Date'].apply(parse_date)
    df['Ship Date'] = df['Ship Date'].apply(parse_date)
    
    # å¤„ç†æ•°å€¼å­—æ®µ
    df['Row ID'] = pd.to_numeric(df['Row ID'], errors='coerce').fillna(0).astype(int)
    df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce').fillna(0.0)
    
    # å¤„ç†æ–‡æœ¬å­—æ®µ
    text_columns = ['Order ID', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment',
                   'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID',
                   'Category', 'Sub-Category', 'Product Name']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].fillna('').astype(str)
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å·²æœ‰æ•°æ®
    with conn.cursor() as cur:
        cur.execute("SELECT COUNT(*) FROM store_sales_data")
        existing_count = cur.fetchone()[0]
        
        if existing_count > 0:
            logger.warning(f"âš ï¸  è¡¨ä¸­å·²æœ‰ {existing_count} æ¡æ•°æ®")
            response = input("æ˜¯å¦æ¸…ç©ºç°æœ‰æ•°æ®å¹¶é‡æ–°å¯¼å…¥ï¼Ÿ(y/N): ")
            if response.lower() == 'y':
                cur.execute("TRUNCATE TABLE store_sales_data")
                conn.commit()
                logger.info("âœ… å·²æ¸…ç©ºç°æœ‰æ•°æ®")
            else:
                logger.info("è·³è¿‡å¯¼å…¥ï¼Œä¿ç•™ç°æœ‰æ•°æ®")
                return
    
    # æ‰¹é‡æ’å…¥æ•°æ®
    logger.info(f"ğŸ“¤ å¼€å§‹æ‰¹é‡å¯¼å…¥æ•°æ®ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼‰...")
    
    insert_sql = """
    INSERT INTO store_sales_data (
        row_id, order_id, order_date, ship_date, ship_mode,
        customer_id, customer_name, segment, country, city, state,
        postal_code, region, product_id, category, sub_category,
        product_name, sales
    ) VALUES %s
    """
    
    total_rows = len(df)
    inserted_rows = 0
    
    try:
        with conn.cursor() as cur:
            for i in range(0, total_rows, batch_size):
                batch = df.iloc[i:i+batch_size]
                
                # å‡†å¤‡æ•°æ®
                values = [
                    (
                        int(row['Row ID']) if pd.notna(row['Row ID']) else None,
                        str(row['Order ID']),
                        row['Order Date'] if pd.notna(row['Order Date']) else None,
                        row['Ship Date'] if pd.notna(row['Ship Date']) else None,
                        str(row['Ship Mode']),
                        str(row['Customer ID']),
                        str(row['Customer Name']),
                        str(row['Segment']),
                        str(row['Country']),
                        str(row['City']),
                        str(row['State']),
                        str(row['Postal Code']),
                        str(row['Region']),
                        str(row['Product ID']),
                        str(row['Category']),
                        str(row['Sub-Category']),
                        str(row['Product Name']),
                        float(row['Sales']) if pd.notna(row['Sales']) else 0.0
                    )
                    for _, row in batch.iterrows()
                ]
                
                # æ‰¹é‡æ’å…¥
                execute_values(cur, insert_sql, values)
                conn.commit()
                
                inserted_rows += len(batch)
                progress = (inserted_rows / total_rows) * 100
                logger.info(f"ğŸ“ˆ è¿›åº¦: {inserted_rows}/{total_rows} ({progress:.1f}%)")
        
        logger.info(f"âœ… æ•°æ®å¯¼å…¥å®Œæˆï¼å…±å¯¼å…¥ {inserted_rows} æ¡è®°å½•")
        
    except Exception as e:
        conn.rollback()
        logger.error(f"âŒ æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    # CSV æ–‡ä»¶è·¯å¾„
    csv_path = project_root / "raw_data" / "store_sales_data.csv"
    
    if not csv_path.exists():
        logger.error(f"âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        sys.exit(1)
    
    logger.info("ğŸš€ å¼€å§‹å¯¼å…¥ store_sales_data.csv åˆ°æ•°æ®åº“")
    logger.info(f"ğŸ“ CSV æ–‡ä»¶è·¯å¾„: {csv_path}")
    
    # è¿æ¥æ•°æ®åº“
    conn = None
    try:
        conn = get_db_connection()
        
        # åˆ›å»ºè¡¨
        create_table(conn)
        
        # å¯¼å…¥æ•°æ®
        import_data(conn, csv_path)
        
        # éªŒè¯å¯¼å…¥ç»“æœ
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM store_sales_data")
            count = cur.fetchone()[0]
            logger.info(f"âœ… éªŒè¯ï¼šæ•°æ®åº“ä¸­ç°æœ‰ {count} æ¡è®°å½•")
            
            # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            cur.execute("""
                SELECT 
                    COUNT(DISTINCT order_id) as total_orders,
                    COUNT(DISTINCT customer_id) as total_customers,
                    COUNT(DISTINCT product_id) as total_products,
                    MIN(order_date) as earliest_date,
                    MAX(order_date) as latest_date,
                    SUM(sales) as total_sales
                FROM store_sales_data
            """)
            stats = cur.fetchone()
            logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"   - æ€»è®¢å•æ•°: {stats[0]}")
            logger.info(f"   - æ€»å®¢æˆ·æ•°: {stats[1]}")
            logger.info(f"   - æ€»äº§å“æ•°: {stats[2]}")
            logger.info(f"   - æœ€æ—©æ—¥æœŸ: {stats[3]}")
            logger.info(f"   - æœ€æ™šæ—¥æœŸ: {stats[4]}")
            logger.info(f"   - æ€»é”€å”®é¢: ${stats[5]:,.2f}")
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
        sys.exit(1)
    finally:
        if conn:
            conn.close()
            logger.info("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    main()
